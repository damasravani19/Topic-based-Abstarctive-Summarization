Will artificial intelligence destroy humanity? Here are 5 reasons not to worry.
Is the AI apocalypse near? Movies like the Terminator franchise and the Matrix have long portrayed dystopian futures where computers develop superhuman intelligence and destroy the human race — and there are also thinkers who think this kind of scenario is a real danger. We interviewed one of them, Oxford philosopher  , last year. Others include   theorist   and  , an economist at George Mason University. But these thinkers overestimate the likelihood that we'll have computers as smart as human beings and exaggerate the danger that such computers would pose to the human race. In reality, the development of intelligent machines is likely to be a slow and gradual process, and computers with superhuman intelligence, if they ever exist, will need us at least as much as we need them. Here's why.   
 ( ) To see why, imagine taking a brilliant English speaker who has never spoken a word of Chinese, locking her in a room with an enormous stack of books about the Chinese language, and asking her to become fluent in speaking Chinese. No matter how smart she is, how long she studies, and how many textbooks she has, she's not going to be able to learn enough to pass herself off as a native Chinese speaker. That's because an essential part of becoming fluent in a language is interacting with other fluent speakers. Talking to natives is the only way to learn local slang, discover subtle shades in  A machine trying to develop human-level intelligence faces a much more severe version of this same problem. A computer program has never grown up in a human family, fallen in love, been cold, hungry or tired, and so forth. In short, they lack a huge amount of the context that allows human beings to relate naturally to one another. And a similar point applies to lots of other problems intelligent machines might tackle, from drilling an oil well to helping people with their taxes.    
 Machines need humans to help maintain complex machinery like oil rigs. ( ) In the Terminator series, a military AI called Skynet becomes self-aware and begins using military hardware to attack humans. This kind of scenario drastically underestimates how much machines depend on human beings to keep them working. A modern economy consists of millions of different kinds of machines that perform a variety of specialized functions. While a growing number of these machines are automated to some extent, virtually all of them depend on humans to supply power and raw materials, repair them when they break, manufacture more when they wear out, and so forth. You might imagine humanity creating still more robots being created to perform these maintenance functions. But we're nowhere close to having this kind of general-purpose robot. Indeed, building such a robot might be impossible due to a problem of  This means that, barring major breakthroughs in robotics or nanotechnology, machines are going to depend on humans for supplies, repairs, and other maintenance. A smart computer that wiped out the human race would be committing suicide.   
 ( ) Bostrom argues that if nothing else, scientists will be able to produce at least human-level intelligence by emulating the human brain, an idea that Hanson has also  . But that's a lot harder than it sounds. Digital computers are capable of emulating the behavior of other digital computers because computers function in a precisely-defined, deterministic way. To simulate a computer, you just have to carry out the sequence of instructions that the computer being modeled would perform. The human brain isn't like this at all. Neurons are complex analog systems whose behavior can't be modeled precisely the way digital circuits can. And even a slight imprecision in the way individual neurons are modeled can lead to a wildly inaccurate model for the brain as a whole. A good analogy here is  . Physicists have an excellent understanding of the behavior of individual air molecules. So you might think we could build a model of the earth's atmosphere that predicts the weather far into the future. But so far, weather simulation has proven to be a computationally intractable problem. Small errors in early steps of the simulation snowball into large errors in later steps. Despite huge increases in computing power over the last couple of decades, we've only made modest progress in being able to predict future weather patterns. Simulating a brain precisely enough to produce intelligence is a much harder problem than simulating a planet's weather patterns. There's no reason to think scientists will be able to do it in the foreseeable future.   
 (White House) Bostrom suggests that intelligent machines could "become extremely powerful to the point of being able to shape the future according to its preferences." But if we think about how human societies work, it's obvious that intelligence by itself isn't sufficient to become powerful. If it were, societies would be run by their scientists, philosophers, or chess prodigies. Instead, America — like most societies around the world — is run by men like Ronald Reagan, Bill Clinton, and George W. Bush. These men became powerful not because they were unusually bright, but because they were well-connected, charismatic, and knew how to offer the right combination of carrots and sticks to get others to do their bidding. It's true that brilliant scientists have played an important role in creating powerful technologies such as the atomic bomb. And it's conceivable that a super intelligent computer would conceive of similar breakthroughs. But building new technologies and putting them into practice usually requires a lot of cash and manpower, which only powerful institutions like governments and large corporations can muster. The scientists who designed the atomic bomb needed Franklin Roosevelt to fund it. The same point applies to intelligent computers. Any plausible plan for taking over the world would require the cooperation of thousands of people. There's no reason to think a computer would be any more effective at enlisting their assistance for an evil plot than a human scientist would be. Indeed, given that persuasion often depends on long-standing friendships, in-group loyalties, and charisma, a disembodied, friendless computer program would be at a huge disadvantage. A similar point applies to the "singularly," Ray Kurzweil's idea that computers will someday become so intelligent that humans will no longer even be able to understand what they're doing. The most powerful ideas aren't ones that only their inventor can understand. Rather, powerful ideas are ones that can be widely understood and adopted by many people, multiplying their effect on the world. That will be as true of computer-generated ideas as it is of ideas generated by people. To change the world, a super-intelligent computer would need to bring the human race along with it.   
 You might expect that computers will use their superior intelligence to become fabulously wealthy and then use their vast wealth to bribe humans into doing their bidding.  So the first super-intelligent computer might be able to earn a lot of money, but its advantage will be fleeting. As computer chips continue getting cheaper and more powerful, people will build more and more super-intelligent computers. The unique capabilities of super-intelligent computers, whatever those turn out to be, will become commodities. In a world of abundant intelligence, the most valuable resources will be those that are naturally limited, like land, energy, and minerals. Since those resources are controlled by human beings, we'll have at least as much leverage over intelligent computers as they'll have over us. Get our newsletter in your inbox twice a week. 